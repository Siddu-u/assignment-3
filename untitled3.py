# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Y9Wtxl2q63fe7T_gWnWt2piBGyL79dcA
"""

# STEP 1: Load the text dataset
import tensorflow as tf
import numpy as np
import os

# Download and read the text data
path = tf.keras.utils.get_file("little_prince.txt", "https://www.gutenberg.org/files/1417/1417-0.txt")
text = open(path, encoding='utf-8').read().lower()
text = text[:100000]  # ðŸ”» Use only the first 100,000 characters for faster training
print(f"Reduced text length: {len(text)} characters")

# STEP 2: Preprocess the data
vocab = sorted(set(text))
char2idx = {char: idx for idx, char in enumerate(vocab)}
idx2char = np.array(vocab)
text_as_int = np.array([char2idx[c] for c in text])

# Create training examples and targets
seq_length = 100
char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)
sequences = char_dataset.batch(seq_length+1, drop_remainder=True)

def split_input_target(chunk):
    return chunk[:-1], chunk[1:]

dataset = sequences.map(split_input_target)

# STEP 3: Build the LSTM model (optimized)
BATCH_SIZE = 32  # ðŸ”» Reduced
BUFFER_SIZE = 10000
EMBEDDING_DIM = 128  # ðŸ”» Reduced
RNN_UNITS = 256      # ðŸ”» Reduced
VOCAB_SIZE = len(vocab)

dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)

def build_model(vocab_size, embedding_dim, rnn_units, batch_size):
    return tf.keras.Sequential([
        tf.keras.layers.Input(shape=(None,), batch_size=batch_size),
        tf.keras.layers.Embedding(vocab_size, embedding_dim),
        tf.keras.layers.LSTM(rnn_units, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'),
        tf.keras.layers.Dense(vocab_size)
    ])

model = build_model(VOCAB_SIZE, EMBEDDING_DIM, RNN_UNITS, BATCH_SIZE)

# STEP 4: Compile and train the model
def loss(labels, logits):
    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)

model.compile(optimizer='adam', loss=loss)

# Checkpoint setup
checkpoint_dir = './training_checkpoints'
checkpoint_prefix = os.path.join(checkpoint_dir, "ckpt_{epoch}.weights.h5")

checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(
    filepath=checkpoint_prefix,
    save_weights_only=True
)

EPOCHS = 1  # ðŸ”» Reduced to 1 epoch for quick testing
history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])

# STEP 5: Text Generation with Temperature Scaling
def generate_text(model, start_string, temperature=1.0):
    model = build_model(VOCAB_SIZE, EMBEDDING_DIM, RNN_UNITS, batch_size=1)
    model.load_weights(os.path.join(checkpoint_dir, "ckpt_1.weights.h5"))
    model.build(tf.TensorShape([1, None]))

    input_eval = [char2idx[s] for s in start_string.lower()]
    input_eval = tf.expand_dims(input_eval, 0)

    text_generated = []

    for i in range(500):
        predictions = model(input_eval)
        predictions = tf.squeeze(predictions, 0)

        predictions = predictions / temperature
        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()

        input_eval = tf.expand_dims([predicted_id], 0)
        text_generated.append(idx2char[predicted_id])

    return start_string + ''.join(text_generated)

# Generate sample outputs
print("\n--- Generated Text with Temperature = 0.5 ---\n")
print(generate_text(model, start_string="Once upon a time ", temperature=0.5))

print("\n--- Generated Text with Temperature = 1.0 ---\n")
print(generate_text(model, start_string="Once upon a time ", temperature=1.0))

import nltk
# Download all essential resources safely
nltk.download('punkt')
nltk.download('punkt_tab')
nltk.download('stopwords')
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer

# Download required NLTK data (only once)
nltk.download('punkt')
nltk.download('stopwords')

def preprocess_nlp(sentence):
    # Step 1: Tokenization
    tokens = word_tokenize(sentence)
    print("1. Original Tokens:")
    print(tokens)

    # Step 2: Remove Stopwords
    stop_words = set(stopwords.words('english'))
    tokens_without_stopwords = [word for word in tokens if word.lower() not in stop_words]
    print("\n2. Tokens Without Stopwords:")
    print(tokens_without_stopwords)

    # Step 3: Stemming
    stemmer = PorterStemmer()
    stemmed_tokens = [stemmer.stem(word) for word in tokens_without_stopwords]
    print("\n3. Stemmed Words:")
    print(stemmed_tokens)

# Sample sentence
sentence = "NLP techniques are used in virtual assistants like Alexa and Siri."
preprocess_nlp(sentence)

import spacy

# Load English language model
nlp = spacy.load("en_core_web_sm")

# Input sentence
sentence = "Barack Obama served as the 44th President of the United States and won the Nobel Peace Prize in 2009."

# Apply NLP pipeline
doc = nlp(sentence)

# Extract and print entities
for ent in doc.ents:
    print(f"Text: {ent.text}, Label: {ent.label_}, Start: {ent.start_char}, End: {ent.end_char}")

import numpy as np

def scaled_dot_product_attention(Q, K, V):
    # Step 1: Compute dot product of Q and Káµ€
    matmul_qk = np.dot(Q, K.T)

    # Step 2: Scale by sqrt(d), where d is the key dimension
    d_k = K.shape[-1]
    scaled_scores = matmul_qk / np.sqrt(d_k)

    # Step 3: Apply softmax to get attention weights
    def softmax(x):
        e_x = np.exp(x - np.max(x, axis=-1, keepdims=True))
        return e_x / e_x.sum(axis=-1, keepdims=True)

    attention_weights = softmax(scaled_scores)

    # Step 4: Multiply attention weights by V
    output = np.dot(attention_weights, V)

    return attention_weights, output

# Test inputs
Q = np.array([[1, 0, 1, 0], [0, 1, 0, 1]])
K = np.array([[1, 0, 1, 0], [0, 1, 0, 1]])
V = np.array([[1, 2, 3, 4], [5, 6, 7, 8]])

# Run the function
attention_weights, output = scaled_dot_product_attention(Q, K, V)

# Print results
print("1. Attention Weights:\n", attention_weights)
print("\n2. Final Output:\n", output)

# Sentiment Analysis using HuggingFace Transformers

from transformers import pipeline

# Load a pre-trained sentiment analysis pipeline
sentiment_pipeline = pipeline("sentiment-analysis", model="distilbert-base-uncased-finetuned-sst-2-english")

# Input sentence
sentence = "Despite the high price, the performance of the new MacBook is outstanding."

# Analyze sentiment
result = sentiment_pipeline(sentence)[0]

# Display results
print(f"Sentiment: {result['label']}")
print(f"Confidence Score: {round(result['score'], 4)}")